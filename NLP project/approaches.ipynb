{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Normal_Approach.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"1ruX3e57o_Y6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b93b21b1-94d6-4f67-d09b-1a902205af96","executionInfo":{"status":"ok","timestamp":1541611286918,"user_tz":-60,"elapsed":1123,"user":{"displayName":"mohamed ayoub ben ayad","photoUrl":"https://lh4.googleusercontent.com/-GA3MrdQBOkM/AAAAAAAAAAI/AAAAAAAAABE/8bEE1SZ_Lh8/s64/photo.jpg","userId":"05001042391798275148"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"wpe_RiixwI7A","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install tabulate"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QaMGVWvdwQvp","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install gensim"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1sZKHg1Bvp7E","colab_type":"code","colab":{}},"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n","nltk.download(\"stopwords\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4mTIdQruOsJe","colab_type":"code","colab":{}},"cell_type":"code","source":["import sys\n","\n","\n","%matplotlib inline\n","\n","# basic python libraries\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","\n","from collections import Counter, defaultdict\n","\n","from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n","from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n","from sklearn.svm import SVC\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder,StandardScaler\n","\n","from sklearn.cross_validation import cross_val_score\n","from sklearn.cross_validation import StratifiedShuffleSplit\n","\n","from sklearn.pipeline import Pipeline\n","\n","from tabulate import tabulate\n","\n","import struct\n","\n","# NLP /related /libraries\n","\n","from gensim.models.word2vec import Word2Vec\n","from nltk.stem import PorterStemmer\n","import regex as re\n","\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk import SnowballStemmer\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","lmt = WordNetLemmatizer()\n","stemmer = SnowballStemmer(\"english\")\n","ps = PorterStemmer()\n","\n","\n","stop_words = stopwords.words('english')\n","\n","\n","train_data = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/train.csv\")\n","\n","test_data = pd.read_csv(\"gdrive/My Drive/test.csv\")\n","\n","y = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/train.csv\").Label\n","\n","\n","\n","train_tweets = train_data.TweetText[:]\n","test_tweets = test_data.TweetText[:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NXwAPJmaZotu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"6bc78bef-dfd7-4d3c-addd-0be1a3fea3bc","executionInfo":{"status":"ok","timestamp":1541614747073,"user_tz":-60,"elapsed":5981,"user":{"displayName":"mohamed ayoub ben ayad","photoUrl":"https://lh4.googleusercontent.com/-GA3MrdQBOkM/AAAAAAAAAAI/AAAAAAAAABE/8bEE1SZ_Lh8/s64/photo.jpg","userId":"05001042391798275148"}}},"cell_type":"code","source":["FLAGS = re.MULTILINE | re.DOTALL\n","\n","def hashtag(text) :\n","    text = text.group()\n","    hashtag_body = text[1:]\n","    if hashtag_body.isupper():\n","        result = \" {} \".format(hashtag_body.lower())\n","    else:\n","        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n","    return result\n","\n","  \n","def allcaps(text) :\n","    text = text.group()\n","    return text.lower() + \" <allcaps>\"\n","  \n","def apostr(s) :\n","  if s[0]==\"\\'\" :\n","    return s[1:]\n","  elif s[len(s)-1]==\"\\'\" :\n","    return s[:len(s)-1]\n","  else :\n","    return s\n","\n","\n","def tokenize(text):\n","    # Different regex parts for smiley faces\n","    \n","    eyes = r\"[8:=;]\"\n","    nose = r\"['`\\-]?\"\n","\n","    # function so code less repetitive\n","    def re_sub(pattern, repl):\n","        return re.sub(pattern, repl, text, flags=FLAGS)\n","\n","    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n","    \n","    #text = re_sub(r\"@\\w+\", \" \")  # removing this ones gave a good accuracy boost\n","    \n","    \n","    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n","    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n","    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n","    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n","    \n","    \n","    \n","    text = re_sub(r\"/\",\" / \")\n","    \n","    text = re_sub(r\"<3\",\"love\")\n","    \n","    text = re_sub(r\"\\\\\",\" \")\n","    \n","    # numbers / hashtags / elong / allcaps\n","    \n","    \n","    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n","    text = re_sub(r\"#\\S+\", hashtag)\n","    \n","    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n","    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n","    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n","    \n","    # what I aded\n","    \n","    text = re_sub(r\"[\\,\\:\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\_\\+\\|\\~\\=\\-\\{\\}\\[\\]\\?\\/\\.\\\"\\;`]\", \" \") # punc\n","    \n","    \n","    #text = re_sub(r\"[^\\w\\s\\<\\>\\']\", \" \")\n","\n","    text = re_sub(r\"[\\<]\", \" <\")\n","    text = re_sub(r\"[\\>]\", \"> \")\n","    \n","    # dealing with \\'\n","    \n","    text = re_sub(r\"\\'ll\", \" will \") # treating  i'll and we'll ..\n","    text = re_sub(r\"\\'s\", \" \")\n","    text = re_sub(r\"\\'m\", \" am\")\n","    text = re_sub(r\"\\'d\", \" would\")\n","    text = re_sub(r\"\\'ve\", \" have\")\n","    text = re_sub(r\"\\'re\", \" are\")\n","    text = re_sub(r\"can't\", \" can not\")\n","    text = re_sub(r\"\\'t\", \" not\")\n","    text = re_sub(r\"fc \",\" football club\")\n","    \n","    \n","    return text.lower()\n","  \n","  \n","  \n","  \n","def tokenize_(liste_tweets) :\n","  \n","  tokens = [re.findall(r\"\\S+\",tokenize(i)) for i in liste_tweets]\n","  \n","  \n","  # here should happen the mapping :\n","  \n","  \n","  # lematizing / steming / stop words\n","  \n","  for i in range(len(tokens)) :\n","      #tokens[i] = [ lmt.lemmatize(j) for j in tokens[i] ] # lemmatizing    \n","      #tokens[i] = [ ps.stem(j) for j in tokens[i] ] # stemming    \n","      tokens[i] = [ j for j in tokens[i] if j not in stop_words ] # deleting stop words should make a list words specified to twitter \n","      \n","      \n","  \n","  \n","  # dealing with \\'\n","  \n","  for i in range(len(tokens)) :\n","    for j in range(len(tokens[i])) :\n","      if \"\\'\" in tokens[i][j] :\n","        tokens[i][j] = apostr(tokens[i][j])\n","  \n","  # cleaning some noise :\n","  \n","  for i in range(len(tokens))  :\n","      tokens[i] = [k for k in tokens[i] if len(k)>0 ]\n","      \n","  \n","  \n","  return tokens\n","\n","\n","  \n","\n","X = tokenize_(train_tweets) \n","\n","\n","  \n","\n","test_tokens = tokenize_(test_tweets) \n","\n","\n","map_words(X)\n","\n","\n","  \n","\n","map_words(test_tokens)\n","\n","\n","X_ = X + test_tokens\n","\n","\n","mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n","#bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x,ngram_range=(1,1))), (\"bernoulli nb\", BernoulliNB())])\n","mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n","#bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x,ngram_range=(1,1))), (\"bernoulli nb\", BernoulliNB())])\n","#svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x,ngram_range=(1,1))), (\"linear svc\", SVC(C=10,gamma=1))])\n","#svc_ = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x,ngram_range=(1, 3))), (\"linear svc\", SVC(C=10,gamma=1))])\n","\n","\n","all_models = [\n","    (\"mult_nb\", mult_nb),\n","    (\"mult_nb_tfidf\", mult_nb_tfidf),\n","]\n","\"\"\"(\"bern_nb\", bern_nb),\n","    (\"bern_nb_tfidf\", bern_nb_tfidf),\n","    (\"svc_tfidf\", svc_tfidf)\"\"\"\n","\n","unsorted_scores = [(name, cross_val_score(model, X, y, cv=10).mean()) for name, model in all_models]\n","scores = sorted(unsorted_scores, key=lambda x: -x[1])\n","\n","\n","print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"],"execution_count":59,"outputs":[{"output_type":"stream","text":["model            score\n","-------------  -------\n","mult_nb         0.9629\n","mult_nb_tfidf   0.9603\n"],"name":"stdout"}]},{"metadata":{"id":"mjEP9Gua_k_V","colab_type":"code","colab":{}},"cell_type":"code","source":["test_tokens"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_sWpVogKgGkc","colab_type":"code","colab":{}},"cell_type":"code","source":["def submit_(string_,clf) :\n","  test_id = test_data.TweetId\n","  clf.fit(X,y)\n","  prediction = clf.predict(test_tokens)\n","  df = pd.DataFrame(data={\"TweetId\":test_id,\"Label\":prediction})\n","  df.to_csv(string_,index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YOwO_uo2jeV_","colab_type":"code","colab":{}},"cell_type":"code","source":["submit_(\"sub_with_mapping_test_tokens2.csv\",mult_nb)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4swKoM-jJ_eb","colab_type":"code","colab":{}},"cell_type":"code","source":["def find_s(X_,str_,int_) :\n","  for i in range(len(X_)) :\n","    for j in X_[i] :\n","      if str_ in j :\n","        if int_ == 1 :\n","          if j == str_ :\n","            if i<len(X) :\n","               print(j,\" >>>>> \",y[i])\n","            else :\n","              print(j,\" >>>>> \")          \n","        else : \n","          if i<len(X) :\n","            print(j,\" >>>>> \",y[i])\n","          else :\n","            print(j,\" >>>>> \")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hN8BFzQmd72Y","colab_type":"code","colab":{}},"cell_type":"code","source":["find_s(X_,\"mcfc\",0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dQ8uSil4wwlv","colab_type":"code","colab":{}},"cell_type":"code","source":["def predict_(X,y) :\n","  return cross_val_score(mult_nb,X,y,cv=5).mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qoc961oEshnZ","colab_type":"code","colab":{}},"cell_type":"code","source":["xtr,xte,ytr,yte = train_test_split(X,y,test_size=0.2)\n","\n","\n","dictio = {}\n","\n","yy = list(yte)\n","\n","predict_pro = [0 for i in xte]\n","\n","for i in range(len(xte)) :\n","  dictio[i]=0\n","  \n","err_gen = []\n","\n","\n","def predict_2(clf) :\n","  err = 0\n","  clf.fit(xtr,ytr)\n","  \n","  pred = clf.predict(xte)\n","  pred2 = clf.predict_proba(xte)\n","  \n","  # proba = clf.predict_proba(xte)\n","  for i in range(len(xte)) :             \n","    if yy[i] != pred[i] :\n","      dictio[i]+=1\n","  return pred2\n","\n","models = [\n","     mult_nb,\n","     mult_nb_tfidf\n","]\n","\n","\n","for i in models :\n","  pred2 = predict_2(i)\n","  \n","  predict_pro = [ pred2[i] + predict_pro[i] for i in range(len(predict_pro)) ]\n","t1 = []\n","\n","for i in dictio.keys() :\n","  if dictio[i]==2 :\n","    t1 += [i]\n","    print(i , yy[i] , xte[i])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qxqtMG6xCw7Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c8c1f2fd-57a2-476b-ffca-a42ebf811bff","executionInfo":{"status":"ok","timestamp":1541614462889,"user_tz":-60,"elapsed":1333,"user":{"displayName":"mohamed ayoub ben ayad","photoUrl":"https://lh4.googleusercontent.com/-GA3MrdQBOkM/AAAAAAAAAAI/AAAAAAAAABE/8bEE1SZ_Lh8/s64/photo.jpg","userId":"05001042391798275148"}}},"cell_type":"code","source":["find_s(X_,\"mcfc\",0)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["mcfci  >>>>> \n"],"name":"stdout"}]},{"metadata":{"id":"BlU4fzYaPbLH","colab_type":"code","colab":{}},"cell_type":"code","source":["dic_map = {\n","           \"sports\":[\"sports\"],\n","           \"football\":[\"sports\",\"football\",\"basketball\"],\n","           \"ausg\":[\"ausgp\",\"sports\",\"gp\",\"australia\"],\n","           \"tennis\":[\"tennis\",\"sports\"],\n","           \"olympics\":[\"sports\",\"olympics\"],\n","           \"espn\":[\"espn\",\"sports\"],\n","           \"cricket\":[\"cricket\",\"sports\"],\n","           \"fans\" :[\"fans\",\"sport\"],\n","            \n","          \"club\":[\"club\",\"sports\"],\n","    \n","            \"mcfc\":[\"mcfc\",'manchester', 'city', 'football', 'club'],\n","           \"cfc\":[\"cfc\",\"chelsea\",\"football\" , \"club\",\"sports\"],\n","           \"lfc\":[\"lfc\",\"liverpool\",\"football\" , \"club\",\"sports\"],\n","         \n","           \"motogp\":[\"gp\",\"sports\",\"motogp\"],\n","    \n","            \"parli\" :[\"parliament\",\"politics\"],\n","           \"obama\":[\"politics\",\"obama\",\"president\",\"gov\"],\n","           \"clinton\":[\"politics\",\"obama\",\"politics\",\"clinton\"],\n","           \"gov\" :[\"gov\",\"politics\"],\n","           \"minister\":[\"minister\",\"politics\"],\n","           \"african\":[\"african\"],\n","          \"afghan\" :[\"afghanistan\",\"politics\"],\n","           \"europe\":[\"europe\"],\n","           \"peace\":[\"peace\",\"politics\"],\n","           \"kerry\": [\"kerry\",\"politics\"],\n","            \"syria\":[\"politics\"],\n","              \"unsg\":['united','nations' ,'secretary' ,'general','politics']\n","    \n","          }"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v7gHoEECQEHI","colab_type":"code","colab":{}},"cell_type":"code","source":["def map_words(X_) :\n","  for i in range(len(X_)) :\n","    for j in range(len(X_[i])) :\n","\n","      del_ = []\n","      add_ = []\n","\n","      for mapp in dic_map :\n","        if mapp in X_[i][j] :\n","          del_ += [X_[i][j]]\n","          add_ +=  dic_map[mapp]\n","          break\n","\n","      for elm in del_ :\n","        X_[i].remove(elm)\n","\n","      X_[i]+=add_\n","    X_[i] = [j for j in X_[i] if len(j)>1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wkFPHiBBqwdL","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"5MnZXXun-xXj","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"for i in range(len(train_tokens))  :\n","  l = []\n","  for w in train_tokens[i]:\n","    if w not in glove_small :\n","      l+=[w]\n","  for elmt in l :\n","    train_tokens[i].remove(elmt)\n","      \n","for i in range(len(test_tokens))  :\n","  l = []\n","  for w in test_tokens[i]:\n","    if w not in glove_small :\n","      l+=[w]\n","  for elmt in l :\n","    test_tokens[i].remove(elmt)\"\"\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"QLMx0IUHZQ_n","colab_type":"code","colab":{}},"cell_type":"code","source":["GLOVE_PATH = \"gdrive/My Drive/glove.twitter.27B.200d.txt\"\n","\n","encoding=\"utf-8\"\n","    \n","glove_small = {}\n","\n","with open(GLOVE_PATH, \"rb\") as infile:\n","    for line in infile:\n","        parts = line.split()\n","        word = parts[0].decode(encoding)\n","        nums=np.array(parts[1:], dtype=np.float32)\n","        glove_small[word] = nums"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8BizwIUh2uRi","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","Now >> Classifiers using vector embeddings. \n","We will implement an embedding vectorizer - a counterpart of CountVectorizer and TfidfVectorizer - \n","that is given a word -> vector mapping and vectorizes texts by taking the mean of all the vectors corresponding to individual words.\n","\"\"\"\n","\n","class MeanEmbeddingVectorizer(object):\n","    def __init__(self, word2vec):\n","        self.word2vec = word2vec\n","        if len(word2vec)>0:\n","            self.dim=200\n","        else:\n","            self.dim=0\n","            \n","    def fit(self, X, y):\n","        return self \n","\n","    def transform(self, X):\n","        return np.array([\n","            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n","                    or [np.zeros(self.dim)], axis=0)\n","            for words in X\n","        ])\n","\n","    \n","# tf-idf version of the same\n","class TfidfEmbeddingVectorizer(object):\n","    def __init__(self, word2vec):\n","        self.word2vec = word2vec\n","        self.word2weight = None\n","        if len(word2vec)>0:\n","            self.dim=200\n","        else:\n","            self.dim=0\n","        \n","    def fit(self, X, y):\n","        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n","        tfidf.fit(X)\n","        # if a word was never seen - it must be at least as infrequent\n","        # as any of the known words - so the default idf is the max of \n","        # known idf's\n","        max_idf = max(tfidf.idf_)\n","        self.word2weight = defaultdict(\n","            lambda: max_idf, \n","            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n","    \n","        return self\n","    \n","    def transform(self, X):\n","        return np.array([\n","                np.mean([self.word2vec[w] * self.word2weight[w]\n","                         for w in words if w in self.word2vec] or\n","                        [np.zeros(self.dim)], axis=0)\n","                for words in X\n","            ])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IwtDgEBC2yfI","colab_type":"code","colab":{}},"cell_type":"code","source":["# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n","\n","\n","etree_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n","                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n","\n","\n","etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n","                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n","\n","\n","rfc_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n","                       (\"rfc\", RandomForestClassifier(n_estimators=200)) ])\n","\n","\n","all_models = [\n","    (\"rfc_glove_small_tfidf\", rfc_glove_small_tfidf),\n","    (\"glove_small\", etree_glove_small),\n","    (\"glove_small_tfidf\", etree_glove_small_tfidf),\n","    \n","    \n","]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WgtYrjw323QT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"58dcee17-73f2-4256-8f41-7db3137debdb","executionInfo":{"status":"ok","timestamp":1541559460954,"user_tz":-60,"elapsed":337250,"user":{"displayName":"mohamed ayoub ben ayad","photoUrl":"https://lh4.googleusercontent.com/-GA3MrdQBOkM/AAAAAAAAAAI/AAAAAAAAABE/8bEE1SZ_Lh8/s64/photo.jpg","userId":"05001042391798275148"}}},"cell_type":"code","source":["unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n","scores = sorted(unsorted_scores, key=lambda x: -x[1])\n","\n","print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"],"execution_count":61,"outputs":[{"output_type":"stream","text":["model                    score\n","---------------------  -------\n","glove_small             0.9358\n","glove_small_tfidf       0.9333\n","rfc_glove_small_tfidf   0.9281\n","svm_glove_small_tfidf   0.5346\n"],"name":"stdout"}]},{"metadata":{"id":"_ypKI6XU4FWb","colab_type":"code","colab":{}},"cell_type":"code","source":["def find_s2(str_,int_) :\n","  l=[]\n","  for i in range(len(X_)) :\n","    for j in X_[i] :\n","      if str_ in j :\n","        if int_ == 1 :\n","          if j == str_ :\n","             l+=[j]      \n","        else : \n","            l+=[j]\n","  return l"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DJCn4JGANKVB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":642},"outputId":"df17b863-7983-485d-9da9-9a7ce7f119e0","executionInfo":{"status":"ok","timestamp":1541552169561,"user_tz":-60,"elapsed":674,"user":{"displayName":"mohamed ayoub ben ayad","photoUrl":"https://lh4.googleusercontent.com/-GA3MrdQBOkM/AAAAAAAAAAI/AAAAAAAAABE/8bEE1SZ_Lh8/s64/photo.jpg","userId":"05001042391798275148"}}},"cell_type":"code","source":["find_s(X_,\"ucl\",0)"],"execution_count":194,"outputs":[{"output_type":"stream","text":["angelahucles  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","angelahucles  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","angelahucles  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","angelahucles  >>>>>  Politics\n","angelahucles  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","maximucle  >>>>>  Sports\n","angelahucles  >>>>>  Politics\n","nuclear  >>>>>  Politics\n","angelahucles  >>>>>  Politics\n","nuclear  >>>>> \n","nurbanuclk  >>>>> \n","nuclear  >>>>> \n","nuclear  >>>>> \n","nuclear  >>>>> \n"],"name":"stdout"}]},{"metadata":{"id":"XfwhNcK56KR-","colab_type":"code","colab":{}},"cell_type":"code","source":["vocab = []\n","for i in X_ :\n","  for j in i :\n","    if j not in vocab :\n","      vocab += [j]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uQISX8MALgxG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"outputId":"bf7f814e-86cf-4c2b-fa49-8602c0a159b2","executionInfo":{"status":"error","timestamp":1541557376539,"user_tz":-60,"elapsed":2384,"user":{"displayName":"mohamed ayoub ben ayad","photoUrl":"https://lh4.googleusercontent.com/-GA3MrdQBOkM/AAAAAAAAAAI/AAAAAAAAABE/8bEE1SZ_Lh8/s64/photo.jpg","userId":"05001042391798275148"}}},"cell_type":"code","source":["for i in  vocab :\n","  if \"\\'\" in i :\n","    print(i)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-197db8c6fff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mvocab\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m\"\\'\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"]}]},{"metadata":{"id":"W0_Eniud6WAu","colab_type":"code","colab":{}},"cell_type":"code","source":["sorted(vocab)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MHmJiQb23q2K","colab_type":"code","colab":{}},"cell_type":"code","source":["for i in X_ :\n","  for j in i :\n","    if len(j) > 3 :\n","      l = find_s2(j,0)\n","      if len(l)> 5 :\n","        print(l)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ltgOEFTL3-x9","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}