{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tokenizer.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"usOvSv8OIENo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"dc64949e-c391-4a7e-a6c4-d0d6d5d09a3b","executionInfo":{"status":"ok","timestamp":1542728291058,"user_tz":-60,"elapsed":431,"user":{"displayName":"mohamed ayoub ben ayad","photoUrl":"https://lh4.googleusercontent.com/-GA3MrdQBOkM/AAAAAAAAAAI/AAAAAAAAABE/8bEE1SZ_Lh8/s64/photo.jpg","userId":"05001042391798275148"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"hYtE9gU7TqA4","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install gensim\n","!pip install tabulate"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F9YpVDmuIKPf","colab_type":"code","colab":{}},"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n","nltk.download(\"stopwords\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WOa5pPvbIZvU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"b9bc73d7-d1e4-4bba-dbb2-aae8399dff3f","executionInfo":{"status":"ok","timestamp":1542728296077,"user_tz":-60,"elapsed":964,"user":{"displayName":"mohamed ayoub ben ayad","photoUrl":"https://lh4.googleusercontent.com/-GA3MrdQBOkM/AAAAAAAAAAI/AAAAAAAAABE/8bEE1SZ_Lh8/s64/photo.jpg","userId":"05001042391798275148"}}},"cell_type":"code","source":["import sys\n","\n","\n","%matplotlib inline\n","\n","# basic python libraries\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","\n","\n","# some structures\n","from collections import Counter, defaultdict\n","from tabulate import tabulate\n","import struct\n","\n","#some models\n","from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n","from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n","from sklearn.svm import SVC\n","\n","\n","# for preprocessing / model evaluation ...\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder,StandardScaler\n","from sklearn.cross_validation import cross_val_score\n","from sklearn.cross_validation import StratifiedShuffleSplit\n","\n","# for productivity\n","from sklearn.pipeline import Pipeline\n","\n","\n","\n","# NLP /related /libraries\n","\n","from gensim.models.word2vec import Word2Vec\n","\n","# for cleaning ..\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk import SnowballStemmer\n","\n","import regex as re\n","\n","\n","\n","# some statiscal features ... BOW approach :\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n","  \"This module will be removed in 0.20.\", DeprecationWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"kBtaLC33VmFi","colab_type":"code","colab":{}},"cell_type":"code","source":["stop_words = stopwords.words('english')\n","\n","\n","train_data = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/train.csv\")\n","test_data = pd.read_csv(\"gdrive/My Drive/test.csv\")\n","\n","y = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/train.csv\").Label\n","\n","\n","\n","train_tweets = train_data.TweetText[:]\n","test_tweets = test_data.TweetText[:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"98p4rwFdIfl1","colab_type":"code","colab":{}},"cell_type":"code","source":["FLAGS = re.MULTILINE | re.DOTALL\n","\n","def hashtag(text) :\n","    text = text.group()\n","    hashtag_body = text[1:]\n","    if hashtag_body.isupper():\n","        result = \" {} \".format(hashtag_body.lower())\n","    else:\n","        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n","    return result\n","  \n","def allcaps(text) :\n","    text = text.group()\n","    return text.lower() + \" <allcaps>\"\n","  \n","def emojis_tokenizer(text):\n","  eyes = r\"[8:=;]\"\n","  nose = r\"['`\\-]?\"\n","  def re_sub(pattern, repl):\n","    return re.sub(pattern, repl, text, flags=FLAGS)\n","  \n","  text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n","  text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n","  text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n","  text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n","  text = re_sub(r\"<3\",\"love\")\n","  return text\n","\n","def apostr_tokenizer(text):\n","  def re_sub(pattern, repl):\n","    return re.sub(pattern, repl, text, flags=FLAGS)\n","  text = re_sub(r\"\\'ll\", \" will \") \n","  text = re_sub(r\"\\'s\", \" \")\n","  text = re_sub(r\"\\'m\", \" am\")\n","  text = re_sub(r\"\\'d\", \" would\")\n","  text = re_sub(r\"\\'ve\", \" have\")\n","  text = re_sub(r\"\\'re\", \" are\")\n","  text = re_sub(r\"can't\", \" can not\")\n","  text = re_sub(r\"\\'t\", \" not\")\n","  text = re_sub(r\"\\'\\s\", \"  \")\n","  text = re_sub(r\"\\s\\'\", \"  \")\n","  text = re_sub(r\"\\'$\", \"  \")\n","  text = re_sub(r\"^\\'\", \"  \")\n","  return text \n","\n","def fc_mapping(data) :\n","  #  tokens that ends with fc are mapped to => football club \n","  for i in range(len(data)) :\n","    fc = [\"football\",\"club\"]\n","    for j in data[i] :\n","      l = re.findall(\"\\w+fc\",j)\n","      if len(l)>0 :\n","        data[i]+=fc\n","        break\n","        \n","def tokenize(text):\n","    # function so code less repetitive\n","    def re_sub(pattern, repl):\n","        return re.sub(pattern, repl, text, flags=FLAGS)  \n","    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n","    \n","    #text = re_sub(r\"@\\w+\", \" \")  # removing this ones gave a good accuracy boost\n","    \n","    # dealing with emojis :\n","    text = emojis_tokenizer(text)\n","    \n","    \n","    text = re_sub(r\"/\",\" / \")\n","    text = re_sub(r\"\\\\\",\" \")\n","    \n","    # numbers / hashtags / elong / allcaps :\n","    \n","    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n","    text = re_sub(r\"#\\S+\", hashtag)\n","    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n","    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n","    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n","    \n","    # Some additional cleaning :\n","    text = re_sub(r\"[\\,\\:\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\_\\+\\|\\~\\=\\-\\{\\}\\[\\]\\?\\/\\.\\\"\\;`]\", \" \") # punc\n","    text = re_sub(r\"[\\<]\", \" <\")\n","    text = re_sub(r\"[\\>]\", \"> \")\n","    \n","    text = re_sub(r\"fc\\s\", \"fc football club \")\n","    \n","    \n","    # dealing with \\'\n","    text = apostr_tokenizer(text)\n","    \n","    \n","    text = text.lower()\n","    \n","    # some simple mapping :\n","    \n","    if \"gp\" in text :\n","      text = re_sub(r\"gp\\s\", \"gp grand prix \")\n","    if \"fc\" in text :\n","      text = re_sub(r\"fc\\s\", \"fc football club \")\n","        \n","    return text\n","\n","  \n","def tokenize_(liste_tweets) :\n","  tokens = [re.findall(r\"\\S+\",tokenize(i)) for i in liste_tweets]\n"," \n","  #for i in range(len(tokens)) :\n","   #   tokens[i] = [ j for j in tokens[i] if j not in stop_words and len(j)>1 ] \n","      \n","  return tokens"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pt7nGzrRxiYI","colab_type":"code","colab":{}},"cell_type":"code","source":["train_tokens = tokenize_(train_tweets)\n","test_tokens = tokenize_(test_tweets)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_Y3fUWw-xMb0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"949d5425-e9d8-4b7d-d816-6e7e9896ef07","executionInfo":{"status":"ok","timestamp":1542728405845,"user_tz":-60,"elapsed":3058,"user":{"displayName":"mohamed ayoub ben ayad","photoUrl":"https://lh4.googleusercontent.com/-GA3MrdQBOkM/AAAAAAAAAAI/AAAAAAAAABE/8bEE1SZ_Lh8/s64/photo.jpg","userId":"05001042391798275148"}}},"cell_type":"code","source":["mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n","\n","mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n","\n","\n","\n","all_models = [\n","    (\"mult_nb\", mult_nb),\n","    (\"mult_nb_tfidf\", mult_nb_tfidf),\n","]\n","\n","unsorted_scores = [(name, cross_val_score(model, train_tokens, y, cv=10).mean()) for name, model in all_models]\n","scores = sorted(unsorted_scores, key=lambda x: -x[1])\n","\n","\n","print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["model            score\n","-------------  -------\n","mult_nb         0.9585\n","mult_nb_tfidf   0.9577\n"],"name":"stdout"}]},{"metadata":{"id":"d7O8uXnnSniA","colab_type":"code","colab":{}},"cell_type":"code","source":["all_tokens =  train_tokens[:] + test_tokens[:] "],"execution_count":0,"outputs":[]},{"metadata":{"id":"rfBUq7qkvliJ","colab_type":"code","colab":{}},"cell_type":"code","source":["X_tokens = [k for i in all_tokens for k in i]\n","\n","most_500 = Counter(X_tokens).most_common(500)\n","\n","freq_most_used_words = [(i,j,freq_(i),1-freq_(i) )for i,j in most_500]\n","freq_most_used_words_2 = [i for i in freq_most_used_words if ( i[2]>0.94 or i[3]>0.94 ) and \"<\" not in i[0] and i[2]<=1]\n","\n","freq_dict = {i[0]:i[2] for i in freq_most_used_words_2}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XB_nMKyVqMGq","colab_type":"code","colab":{}},"cell_type":"code","source":["print(tabulate(freq_most_used_words_2,headers=(\"Key_Words\",\"Occurencies\",\"Politics\",\"Sports\")))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pPvAWPh0L8r7","colab_type":"code","colab":{}},"cell_type":"code","source":["# augment_meaning(train_tokens)\n","# augment_meaning(test_tokens)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0POKnJ_vcENz","colab_type":"code","colab":{}},"cell_type":"code","source":["def map_and_unify_and_augment(data,weight_list,dict_):\n","  for i in range(len(data)) :\n","    del_ = []\n","    add_ = []\n","    for j in range(len(data[i])) :\n","      for k in weight_list :\n","        if k[0] in data[i][j] :\n","          del_ += [data[i][j]]\n","          add_ +=  dict_[k[0]]\n","          break\n","    print(del_,add_)\n","    for elm in del_ :\n","      data[i].remove(elm)\n","    data[i]+=add_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cufBCUVZc9i4","colab_type":"code","colab":{}},"cell_type":"code","source":["map_and_unify_and_augment(train_tokens,freq_most_used_words_2,unifying_dict)\n","map_and_unify_and_augment(test_tokens,freq_most_used_words_2,unifying_dict)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Kc5WH-PgP1Xn","colab_type":"code","colab":{}},"cell_type":"code","source":["train_tokens = tokenize_2(train_tweets) \n","test_tokens = tokenize_2(test_tweets) "],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"MoQIi0K_v_XY"},"cell_type":"markdown","source":["print (tabulate(politics_counter, floatfmt=\".4f\", headers=(\"KeyWords\", 'Occurencies')))"]},{"metadata":{"colab_type":"code","id":"JL_J3JuDv_Fw","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"5P4-S9O-UqwO","colab_type":"code","colab":{}},"cell_type":"code","source":["submit_(\"nothing2.csv\",mult_nb)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-0Fy3TijRQgF","colab_type":"code","colab":{}},"cell_type":"code","source":["def submit_(string_,clf) :\n","  test_id = test_data.TweetId\n","  clf.fit(train_tokens,y)\n","  prediction = clf.predict(test_tokens)\n","  df = pd.DataFrame(data={\"TweetId\":test_id,\"Label\":prediction})\n","  sub  = df.to_csv(string_,index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FEDVJ9fhUTio","colab_type":"code","colab":{}},"cell_type":"code","source":["def find_s(X_,str_,int_) :\n","  for i in range(len(X_)) :\n","    for j in X_[i] :\n","      if str_ in j :\n","        if int_ == 1 :\n","          if j == str_ :\n","            if i<len(X) :\n","               print(j,\" >>>>> \",y[i])\n","            else :\n","              print(j,\" >>>>> \")          \n","        else : \n","          if i<len(X) :\n","            print(j,\" >>>>> \",y[i])\n","          else :\n","            print(j,\" >>>>> \")\n","            \n","            \n","            \n","def map_words(X_) :\n","  for i in range(len(X_)) :\n","    for j in range(len(X_[i])) :\n","\n","      del_ = []\n","      add_ = []\n","\n","      for mapp in dic_map :\n","        if mapp in X_[i][j] :\n","          del_ += [X_[i][j]]\n","          add_ +=  dic_map[mapp]\n","          break\n","\n","      for elm in del_ :\n","        X_[i].remove(elm)\n","\n","      X_[i]+=add_\n","    X_[i] = [j for j in X_[i] if len(j)>1]            \n","\n","def map_words2(X_) :\n","  for i in range(len(X_)) :\n","    for j in range(len(X_[i])) :\n","\n","      del_ = []\n","      add_ = []\n","\n","      for mapp in dic_map2 :\n","        if mapp in X_[i][j] :\n","          del_ += [X_[i][j]]\n","          add_ +=  dic_map2[mapp]\n","          break\n","\n","      for elm in del_ :\n","        X_[i].remove(elm)\n","\n","      X_[i]+=add_\n","    X_[i] = [j for j in X_[i] if len(j)>1] \n","            \n","dic_map = {\n","           \"football\":[\"football\",\"sports\"],\n","           \"tennis\":[\"tennis\",\"sports\"],\n","           \"basket\":[\"basketball\",\"sports\"] ,  \n","           \"cricket\":[\"cricket\",\"sports\"],\n","    \n","           \"ausg\":[\"ausgp\",\"sports\",\"australian grand prix\"],\n","          \"motogp\":[\"grand\", \"prix\", \"motorcycle\" , \"racing\"],\n","          \"bbl\":[\"bbl\",\"sports\"],\n","           \n","           \"obama\":[\"obama\",\"politics\",\"president\"],\n","           \"gov\" :[\"government\",\"politics\"],\n","           \"kerry\": [\"seckerry\",\"politics\",\"secretary\"],\n","            \"mandela\":[\"mandela\",\"politics\",\"president\"],\n","            \"putin\":[\"putin\",\"politics\",\"president\"],\n","    \n","            \"job\" : [\"jobs\",\"politics\"]\n","          }\n","\n","dic_map2 = {\n","           \"football\":[\"football\"],\n","           \"tennis\":[\"tennis\"],\n","           \"basket\":[\"basketball\"] ,  \n","           \"cricket\":[\"cricket\"],\n","           \"ausg\":[\"ausgp\"],\n","          \"bbl\":[\"bbl\"],\n","          \n","           \"obama\":[\"obama\"],\n","           \"gov\" :[\"government\"],\n","           \"kerry\": [\"seckerry\"],\n","            \"mandela\":[\"mandela\"],\n","           \"job\" : [\"jobs\"]\n","          }"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NaWFM5eHrAsf","colab_type":"code","colab":{}},"cell_type":"code","source":["def find_and_group(word):\n","  parents = []\n","  for i in X_tokens :\n","    if word in i :\n","      parents +=[i]\n","  c = Counter(parents).most_common(5)\n","  print(tabulate(c,headers=(\"Parents\",\"Occurencies\")))\n","  return"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0VxMnrtjD-aN","colab_type":"code","colab":{}},"cell_type":"code","source":["def freq_(s) :\n","  all_ = 0\n","  pol = 0\n","  for i in range(len(train_tokens)) :\n","    for j in train_tokens[i] :\n","      if s in j :\n","        all_ += 1\n","        if y[i] == \"Politics\" :\n","          pol += 1 \n","  if all_ == 0 :\n","    return 10\n","  return pol/all_\n","\n","\n","def augment_meaning(data) :\n","  for i in range(len(data)):\n","    list_add = []\n","    for j in data[i] :\n","      for elmt in freq_dict :\n","        if j == elmt :\n","          pol_freq = freq_dict[elmt]\n","          if pol_freq > 0.9 :\n","            list_add += [\"poltics\"]\n","          else :\n","            list_add +=[\"sports\"]\n","    data[i] += list_add"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qez2YA-Eejgu","colab_type":"code","colab":{}},"cell_type":"code","source":["# automatic solution :\n","\n","for each tag :\n","  check how many times it is counted as politics and sports :\n","    if its more than 95% in either one of them :\n","      delete every string with this tag as a substring and insert it with sports or politics.\n","      \n","     "],"execution_count":0,"outputs":[]}]}